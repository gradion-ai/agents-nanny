{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Blog","text":""},{"location":"2025/08/06/agent-authorization-without-the-pain/","title":"Agent Authorization Without the Pain","text":"<p>Your agent needs to read from your Google Calendar and send emails through Gmail. This seemingly simple requirement quickly becomes complex when you realize you need OAuth flows, token refresh logic, and secure credential storage. Multiply that by every API your agent needs.</p> <p>You shouldn't have to build this infrastructure yourself. Connect your agents to 250+ APIs and 3000+ tools with Model Context Protocol (MCP) and Composio. Composio handles authorization, remote MCP servers and tool execution, while your application focuses on agentic reasoning and tool usage.</p> <p>Note</p> <p>I'm not affiliated with Composio, I'm just a happy user. Their approach supports a useful separation of concerns between agent logic and API integration in agentic applications.</p>"},{"location":"2025/08/06/agent-authorization-without-the-pain/#architecture","title":"Architecture","text":"<p>Your Application</p> <ul> <li>Agents connect to Composio MCP servers and use their tools</li> <li>Agents act on behalf of users that authorized API access</li> <li>Agents focus on reasoning and tool usage, not plumbing</li> </ul> <p>Composio Layer</p> <ul> <li>MCP servers act as protocol bridges to external APIs</li> <li>Each API has an auth config, with a connected account per user</li> <li>Auth tokens are stored securely, supporting OAuth2, API keys, etc.</li> </ul> <p>Key Benefits</p> <ul> <li>No OAuth flows or token management in your code</li> <li>Access 250+ APIs and 3000+ tools through MCP</li> <li>Clean separation between agent logic and API integration</li> </ul>"},{"location":"2025/08/06/agent-authorization-without-the-pain/#example","title":"Example","text":"Your browser does not support the video tag.  <p>The complete code is at github.com/krasserm/agent-auth. What follows is an overview of the key steps needed to authorize an agent to use Google Calendar on behalf of a user:</p> <p>Install the Composio Python library, used to access the Composio REST API.</p> <pre><code>pip install composio-client\n</code></pre> <p>Create an auth configuration for the googlecalendar toolkit, using <code>OAUTH2</code> as <code>authScheme</code>.</p> <pre><code>client = Composio(api_key=os.getenv(\"COMPOSIO_API_KEY\"))\n\nresponse = client.auth_configs.create(\n    toolkit={\"slug\": \"googlecalendar\"},\n    auth_config={\n        \"name\": \"calendar-example\", \n        \"type\": \"use_composio_managed_auth\",\n        \"authScheme\": \"OAUTH2\"\n    }\n)\nauth_config_id = response.auth_config.id\n</code></pre> <p>Add a connected account to the auth config and link a <code>user_id</code> to it. That's the id of a user managed by your application, not by Composio.</p> <pre><code>response = client.connected_accounts.create(\n    auth_config={\"id\": auth_config_id},\n    connection={\"user_id\": \"martin\"},\n)\n</code></pre> <p>Initiate the authorization process by redirecting to an OAuth consent screen in a browser window. After completion, the connected account is authorized to use Google Calendar on behalf of the user who granted access.</p> <pre><code>import webbrowser\n\nwebbrowser.open(response.connection_data.val.redirect_url)\n</code></pre> <p>Create an MCP server for the auth config created in step 2 and specify the tools that should be exposed.</p> <pre><code>result = client.mcp.create(\n    name=\"calendar-mcp-server\",\n    auth_config_ids=[auth_config_id],\n    allowed_tools=[\"GOOGLECALENDAR_FIND_EVENT\"]\n)\n</code></pre> <p>Create an MCP server URL that uses the connected account linked to <code>martin</code>.</p> <pre><code>mcp_url = result.mcp_url.replace(\"transport=sse\", \"user_id=martin\")\n# i.e. https://mcp.composio.dev/composio/server/&lt;uuid&gt;?user_id=martin\n# &lt;uuid&gt; is a string of pattern 12345678-90ab-cdef-1234-567890abcdef\n</code></pre> <p>Configure a Pydantic AI agent with the <code>mcp_url</code> so that it can use Google Calendar on behalf of the user who granted access in step 4.</p> <pre><code>from pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nagent = Agent(\n    'openai:o4-mini',\n    toolsets=[\n        MCPServerStreamableHTTP(mcp_url),  \n    ]\n)\n\nasync with agent:\n    result = await agent.run(\n        \"List my Sep 2025 calendar events\"\n    )\n\nprint(result.output)\n</code></pre>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/","title":"Code Actions as Tools: Evolving Tool Libraries for Agents","text":"<p>Programmatic tool calling is gaining traction in agent development. Instead of emitting one JSON tool call at a time, agents generate executable \"code actions\" that call tools in a sandboxed environment. This pattern is inspired by Apple's CodeAct and appears in many agentic systems. More recent implementations increasingly focus on programmatic calling of MCP tools.</p> <p>These solutions typically generate Python or TypeScript APIs for MCP tools, let an agent write code actions that call these APIs, execute the code in a sandboxed environment, and feed results back to the agent. This improves performance compared to JSON-based approaches, but it often misses an important point: a generated code action can itself become a tool, available for reuse in later code actions.</p> <p>This article fills that gap by presenting an approach for building reusable tools from code actions. When following certain design principles, code actions can be saved in a way that supports efficient discovery, inspection, and reuse in later code actions. This is demonstrated by example using a Claude Code plugin that bundles a code action skill, and the ipybox MCP server for local, sandboxed code execution.</p>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#code-actions-as-reusable-tools","title":"Code Actions as Reusable Tools","text":"<p>Most programmatic tool calling implementations treat code actions as ephemeral: generated, executed, then discarded. But a working code action represents a tested solution. An agent iterates on it based on execution feedback until it produces the desired outcome. If it is then saved in a discoverable format with a callable API, that code action becomes a tool that future code actions can import and compose with other tools.</p> <p>Code actions can also be modified after they have been stored, whether to fix bugs discovered during execution or to add new functionality. The agent thus serves two roles: a domain-specific agent performing the task at hand, and a toolsmith evolving its own capabilities.</p> <p>The key difference from JSON tool calling is mutability. In a JSON-based approach, tools are static: defined at development time and immutable at runtime. With code actions as tools, an agent's tool library can evolve at runtime: tools can be added, modified, or composed based on what the agent learns while working.</p>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#example-composing-github-mcp-server-tools","title":"Example: Composing GitHub MCP Server Tools","text":"<p>The approach is demonstrated using tools from the GitHub MCP server. The task is to retrieve the latest commits from a user's most-starred repositories. This requires composing two tools: <code>search_repositories</code> to find repositories, and <code>list_commits</code> to fetch commits for each repository.</p> <p>Setup</p> <p>For environment setup, plugin installation, and a step-by-step walkthrough of the GitHub example in Claude Code, refer to the plugin documentation. The subsections below provide an overview of the core ideas.</p>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#generating-tool-apis","title":"Generating Tool APIs","text":"<p>To generate a Python API for the GitHub MCP server tools, register the server with ipybox by providing its connection details:</p> User prompt<pre><code>Register this MCP server at ipybox under name github\n{\n  \"url\": \"https://api.githubcopilot.com/mcp/\",\n  \"headers\": {\n    \"Authorization\": \"Bearer ${GITHUB_API_KEY}\"\n  }\n}\n</code></pre> <p>ipybox then generates one module per tool, such as search_repositories or list_commits. Each generated module exposes a <code>run()</code> function to invoke the MCP tool, and a typed input parameter model:</p> search_repositories API<pre><code>class Params(BaseModel):\n    query: str\n    sort: Sort | None = None\n    order: Order | None = None\n    perPage: int | None = None\n\ndef run(params: Params) -&gt; str:\n    # Invokes the MCP tool and returns the result\n    ...\n</code></pre>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#augmenting-tools-with-output-types","title":"Augmenting Tools with Output Types","text":"<p>Many MCP servers, including GitHub's, do not provide output schemas. GitHub MCP tools return JSON strings with undocumented structure. Without knowing that structure in advance, an agent cannot reliably write code that processes outputs inside a code action. The agent would have to call the tool first, bring the raw output back into its context to inspect it, and only then write processing code. That undermines the goal of handling intermediate results inside the execution environment.</p> <p>To address this, we ask the agent to generate an output parser for <code>search_repositories</code>:</p> User prompt<pre><code>Use the codeact skill to generate an output parser for search_repositories\n</code></pre> <p>The agent calls the tool's <code>run()</code> function with sample inputs, inspects representative responses, and then augments the tool API with a structured output type plus a <code>run_parsed()</code> function:</p> search_repositories API augmentation<pre><code>class Repository(BaseModel):\n    name: str\n    stargazers_count: int\n    # ... other fields extracted from actual responses\n\nclass ParseResult(BaseModel):\n    repositories: list[Repository]\n\ndef run_parsed(params: Params) -&gt; ParseResult:\n    # Import parser logic from a separate, generated module\n    from mcpparse.github.search_repositories import parse\n    return parse(run(params))\n</code></pre> <p>Importantly, output parsers are stored separately from the original tool API. This keeps the tool interface clean and avoids mixing parsing implementation details into the interface definition.</p> <p>Once generated, the augmented tool API is immediately usable in code actions. For tools that do provide output schemas, ipybox generates structured output types during tool API generation, making this augmentation step unnecessary.</p>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#composing-tools-in-a-code-action","title":"Composing Tools in a Code Action","text":"<p>With typed outputs available, the agent can now compose tools within a single code action. For a task like</p> User prompt<pre><code>Use the codeact skill to get the latest 5 commits of the 3 github repos\nof torvalds with the most stars. For each repo, output name, stars and\nthe first line of commit messages, and the link to the commit\n</code></pre> <p>the agent generates a code action that combines the <code>search_repositories</code> and <code>list_commits</code> tools.</p> Code action<pre><code>from mcptools.github import search_repositories, list_commits\n\n# Search returns typed Repository objects\nrepos = search_repositories.run_parsed(\n    search_repositories.Params(query=\"user:torvalds\", sort=\"stars\", perPage=3)\n)\n\n# Iterate and call list_commits for each repo, all within one code action\nfor repo in repos.repositories:\n    commits = list_commits.run(\n        list_commits.Params(owner=\"torvalds\", repo=repo.name, perPage=5)\n    )\n    # Printed results are returned to the agent\n    for commit in json.loads(commits):\n        first_line = commit[\"commit\"][\"message\"].split(\"\\n\")[0]\n        print(f\"{repo.name}: {first_line}\")\n</code></pre> <p>This single code action makes four tool calls (one <code>search_repositories</code>, then three <code>list_commits</code>). With JSON-based tool calling, the same workflow requires four separate inference passes, with intermediate results accumulating in the context window.</p> <p>For brevity, the above example parses <code>list_commits</code> output inline. In practice, you should generate an output parser for it as well.</p>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#saving-code-actions-for-reuse","title":"Saving Code Actions for Reuse","text":"<p>Once a code action works, it can be saved as a parameterized tool for later reuse:</p> User prompt<pre><code>Save this as code action under github category with name commits_of_top_repos. \nMake username, top_n_repos and last_n_commits parameters.\n</code></pre> <p>A key design choice here is to separate interface from implementation. The interface is defined in a commits_of_top_repos.api module, the implementation in commits_of_top_repos.impl. The plugin's code action skill instructs the agent to follow this structure.</p> <p>The API module defines a parameterized <code>run()</code> function, output types, and a tool description via a docstring:</p> commits_of_top_repos API<pre><code>class CommitInfo(BaseModel):\n    sha: str\n    message: str\n    url: str\n\nclass RepoCommits(BaseModel):\n    name: str\n    stars: int\n    commits: list[CommitInfo]\n\ndef run(username: str, top_n_repos: int = 3, last_n_commits: int = 5) -&gt; list[RepoCommits]:\n    \"\"\"Get latest commits from a user's top repositories by stars.\"\"\"\n    ...\n</code></pre> <p>During tool discovery, only the API needs inspection. The implementation stays hidden. This saves tokens during discovery and reduces distraction by keeping non-essential implementation details out of the agent's context.</p> <p>New code actions can then import and reuse the saved tool directly:</p> Code action<pre><code>from gentools.github.commits_of_top_repos import run\n\n# Reuse the saved code action as a tool\nresults = run(username=\"torvalds\", top_n_repos=3, last_n_commits=5)\n</code></pre>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#tool-discovery","title":"Tool Discovery","text":"<p>As the number of MCP tool APIs and saved code actions grows, loading them all into the agent's context window upfront becomes impractical. Even a moderately sized tool collection can consume a large fraction of the available context, leaving less room for the actual task.</p> <p>Progressive tool discovery addresses this by deferring tool loading until it is needed. Tools are organized in a package hierarchy on the filesystem that an agent can explore. When a task arrives, the agent searches the filesystem for relevant tools and inspects the APIs of promising candidates. This approach trades some discovery overhead for substantial context savings.</p> <p>The plugin's discovery mechanism is intentionally simple. More sophisticated options are possible, such as hybrid vector and keyword search over tool descriptions, but filesystem-based discovery is a practical starting point. It can scale to hundreds of tools without introducing additional infrastructure dependencies.</p>"},{"location":"2025/12/16/code-actions-as-tools-evolving-tool-libraries-for-agents/#conclusion","title":"Conclusion","text":"<p>Programmatic tool calling brings performance gains over JSON-based approaches, but it reaches its full potential when code actions become first-class tools. In a JSON-based approach, tools are static: defined at development time and immutable at runtime. With code actions as tools, an agent's tool library can evolve at runtime: tools can be added, modified, or composed based on what the agent learns while working. For efficient tool discovery and use, it is important to save code actions with typed interfaces separated from implementations. Agents can then inspect signatures and docstrings without loading implementation details.</p>"},{"location":"2025/11/12/from-single-user-to-multi-party-conversational-ai/","title":"From Single-User to Multi-Party Conversational AI","text":"<p>Single-user AI agents excel at responding to direct queries in one-on-one interactions. A user sends the agent a self-contained query with sufficient context, and the agent processes it directly. Even in group chats, the typical pattern remains the same: users mention the agent with a direct query. This interaction model treats multi-user environments as collections of individual exchanges rather than true multi-party conversations.</p> <p>Multi-party conversational AI systems, on the other hand, must derive agent queries from more complex exchanges between multiple participants. This requires detecting meaningful patterns while knowing when to stay silent. For example, when a conversation stalls on a decision, the system detects that and suggests resolutions based on available agent capabilities. Single-user agents respond to every input, but multi-party AI must engage only when specific patterns emerge.</p>"},{"location":"2025/11/12/from-single-user-to-multi-party-conversational-ai/#architectural-approach","title":"Architectural Approach","text":"<p>Modifying existing single-user agents through fine-tuning or prompting does not scale. The solution lies in enabling single-user agents to participate in multi-party conversations without requiring modification to the agents themselves. This requires a separate integration layer between group chat and downstream agents.</p> <p>This layer detects patterns in group conversations and transforms them into self-contained queries that single-user agents can process. The layer monitors the conversation, identifies when specific engagement criteria are met, and translates the relevant context into actionable agent requests. For example, when a factual contradiction emerges, it initiates a fact check and informs the group of the result.</p> <p>Engagement criteria vary significantly across applications and should evolve based on group feedback. The layer must support flexible definition of custom criteria in natural language, processed by a reasoning model. It must also provide a feedback mechanism to update the criteria. It acts as a specialized adapter (\"group reasoner\") between group chat and downstream AI agents, implementing group engagement logic through pattern detection and query transformation.</p> <p> </p>"},{"location":"2025/11/12/from-single-user-to-multi-party-conversational-ai/#reference-implementations","title":"Reference Implementations","text":"<p>Reference implementations of this architectural pattern are provided by the open-source projects Group Sense, Group Genie, and Hybrid Groups.</p> <p>Group Sense provides the group reasoner component. It detects patterns in group chat messages and transforms them into queries for AI systems. The library supports both shared, single-threaded reasoning and concurrent reasoning across group members. Concurrent reasoners process group context redundantly but scale better to larger, highly active groups.</p> <p>Group Genie combines the group reasoner with an agent integration layer. It enables single-user AI agents to participate in group chat conversations without requiring modification to the agents themselves. It routes generated queries to agents and responses to dynamically determined recipients. Agents can be built on any technology stack and are integrated through a simple interface.</p> <p>Hybrid Groups demonstrates this approach in production environments by integrating Group Genie into Slack and GitHub. A group session corresponds to a thread in Slack or an issue or a pull request in GitHub. The Slack integration supports custom definition of engagement criteria per channel.</p>"},{"location":"2025/11/12/from-single-user-to-multi-party-conversational-ai/#additional-challenges","title":"Additional Challenges","text":"<p>Pattern detection, query transformation, and recipient determination address core requirements for multi-party conversational AI, but additional challenges remain. Agents must be able to act on behalf of individual group members, particularly when following member-specific instructions rather than general group requests.</p> <p>Beyond these implementation concerns, additional research areas are relevant. Further aspects are covered in this selected list of papers:</p> <ul> <li>Overhearing LLM Agents: A Survey, Taxonomy, and Roadmap: AI agents that monitor ambient conversations unobtrusively and provide contextual assistance without interrupting the flow of discussion.</li> <li>Multi-Party Conversational Agents: A Survey: Modeling the mental state of participants, understanding group dialogue content, and predicting conversation flow.</li> <li>Multi-User Chat Assistant (MUCA): Group conversational AI that determines what to say, when to respond, and who to address through three coordinated modules.</li> <li>Multi-User MultiWOZ: Task-Oriented Dialogues among Multiple Users: A method for supporting task-oriented dialogues where multiple users collaboratively make decisions with an agent, including multi-user contextual query rewriting to convert user chats into consumable system queries.</li> </ul>"},{"location":"archive/2025/","title":"2025","text":""},{"location":"category/code-actions/","title":"Code actions","text":""},{"location":"category/programmatic-tool-calling/","title":"Programmatic tool calling","text":""},{"location":"category/group-assistance/","title":"Group assistance","text":""},{"location":"category/agent-authorization/","title":"Agent authorization","text":""}]}